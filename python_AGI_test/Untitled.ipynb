{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0ea7fe-dc63-4fe4-bb2e-113c73def9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e69536e-d266-4467-9840-71087a492767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Barack Obama—with his wife, Michelle—being sworn in as the 44th president of the United States, January 20, 2009. Key events in the life of Barack Obama. Barack Obama (born August 4, 1961, Honolulu, Hawaii, U.S.) is the 44th president of the United States (2009-17) and the first African American to hold the office. The White House, official residence of the president of the United States, in July 2008. The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Barack Obama's father earned an M.A. in Economics from the school in the early 1960s. But Barack Obama Sr.'s decision to pursue his education had consequences. When he moved to Massachusetts, he ... Most common names of U.S. presidents 1789-2021. Published by. Aaron O'Neill , Feb 2, 2024. The most common first name for a U.S. president is James, followed by John and then William. Six U.S ... Barack and Michelle Obama's daughters, Malia and Sasha, grew up in the White House from 2009 to 2017. To most of the world, Barack and Michelle Obama are the former president and first lady of ...\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "search_ans = search.run(\"Obama's first name?\")\n",
    "search_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10210e13-ad8f-4a2c-a405-9c33af6c2e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a simple Python function that calculates and returns the Fibonacci series up to `n` terms:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_series = [0, 1]\n",
      "    \n",
      "    while len(fib_series) < n:\n",
      "        fib_series.append(fib_series[-1] + fib_series[-2])\n",
      "        \n",
      "    return fib_series\n",
      "\n",
      "# Usage example:\n",
      "print(fibonacci(10))\n",
      "```\n",
      "\n",
      "This function starts with the initial two terms `[0, 1]` of the Fibonacci series. It then enters a loop that runs until it has calculated `n` terms in total. During each iteration of the loop, it calculates the next term by adding the last two terms of the sequence together, and adds this new value to the end of the list.\n",
      "\n",
      "Please note, you need to import nothing for this task as it uses only basic Python features.\n",
      "\n",
      "If you want to generate Fibonacci numbers until a certain limit (for example, first 10 numbers that are less than 50), you can adjust the code accordingly:\n",
      "\n",
      "```python\n",
      "def fibonacci(limit):\n",
      "    a, b = 0, 1\n",
      "    \n",
      "    while True:\n",
      "        if b < limit:\n",
      "            yield b\n",
      "            a, b = b, a + b\n",
      "        else:\n",
      "            break\n",
      "\n",
      "# Usage example:\n",
      "for num in fibonacci(50):\n",
      "    print(num)\n",
      "```\n",
      "\n",
      "This code uses `yield` to create an infinite Fibonacci sequence. It will generate numbers until it reaches or exceeds the given limit (`limit`).\n"
     ]
    }
   ],
   "source": [
    "model = Ollama(model=\"qwen2\")\n",
    "a = model.invoke('Write the fibonnaci series in python')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de99b910-5451-4164-8d77-8e841a96549a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2157dd-ca04-41da-8670-ef7465ceae5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlamaCpp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     13\u001b[0m DEFAULT_SEARCH_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     14\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant tasked with improving Google search \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mshould have a question mark at the end: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m QUESTION_PROMPT_SELECTOR \u001b[38;5;241m=\u001b[39m ConditionalPromptSelector(\n\u001b[1;32m     22\u001b[0m     default_prompt\u001b[38;5;241m=\u001b[39mDEFAULT_SEARCH_PROMPT,\n\u001b[1;32m     23\u001b[0m     conditionals\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;28;01mlambda\u001b[39;00m llm: \u001b[38;5;28misinstance\u001b[39m(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mQUESTION_PROMPT_SELECTOR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m prompt\n",
      "File \u001b[0;32m/Volumes/personal/conda_envs/agi/lib/python3.12/site-packages/langchain/chains/prompt_selector.py:39\u001b[0m, in \u001b[0;36mConditionalPromptSelector.get_prompt\u001b[0;34m(self, llm)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get default prompt for a language model.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Prompt to use for the language model.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m condition, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditionals:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcondition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m      5\u001b[0m DEFAULT_LLAMA_SEARCH_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      6\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<<SYS>> \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You are an assistant tasked with improving Google search \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mand each should have a question mark at the end: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m [/INST]\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m DEFAULT_SEARCH_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     14\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant tasked with improving Google search \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mshould have a question mark at the end: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m QUESTION_PROMPT_SELECTOR \u001b[38;5;241m=\u001b[39m ConditionalPromptSelector(\n\u001b[1;32m     22\u001b[0m     default_prompt\u001b[38;5;241m=\u001b[39mDEFAULT_SEARCH_PROMPT,\n\u001b[0;32m---> 23\u001b[0m     conditionals\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;28;01mlambda\u001b[39;00m llm: \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[43mLlamaCpp\u001b[49m), DEFAULT_LLAMA_SEARCH_PROMPT)],\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m prompt \u001b[38;5;241m=\u001b[39m QUESTION_PROMPT_SELECTOR\u001b[38;5;241m.\u001b[39mget_prompt(model)\n\u001b[1;32m     27\u001b[0m prompt\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LlamaCpp' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \\n You are an assistant tasked with improving Google search \\\n",
    "results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that \\\n",
    "are similar to this question. The output should be a numbered list of questions \\\n",
    "and each should have a question mark at the end: \\n\\n {question} [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an assistant tasked with improving Google search \\\n",
    "results. Generate THREE Google search queries that are similar to \\\n",
    "this question. The output should be a numbered list of questions and each \\\n",
    "should have a question mark at the end: {question}\"\"\",\n",
    ")\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(model)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98a4fbb7-ba62-4c95-a2b7-61e34839ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc26d07-315b-43e3-9092-7e3a1b43aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAMELAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_message: SystemMessage,\n",
    "        model: ChatOllama(model=\"qwen2\", format=\"json\", temperature=1.0),\n",
    "    ) -> None:\n",
    "        self.system_message = system_message\n",
    "        self.model = model\n",
    "        self.init_messages()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.init_messages()\n",
    "        return self.stored_messages\n",
    "\n",
    "    def init_messages(self) -> None:\n",
    "        self.stored_messages = [self.system_message]\n",
    "\n",
    "    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n",
    "        self.stored_messages.append(message)\n",
    "        return self.stored_messages\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        input_message: HumanMessage,\n",
    "    ) -> AIMessage:\n",
    "        messages = self.update_messages(input_message)\n",
    "\n",
    "        output_message = self.model.invoke(messages)\n",
    "        self.update_messages(output_message)\n",
    "\n",
    "        return output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41d67c13-ca27-4169-a41f-fb150a534297",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_role_name = \"Python Programmer\"\n",
    "user_role_name = \"Stock Trader\"\n",
    "task = \"Develop a trading bot for the stock market\"\n",
    "word_limit = 50  # word limit for task brainstorming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7819e323-98d6-4d37-9da3-685176c6c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified task: {\n",
      "  \"task\": {\n",
      "    \"description\": \"Design and implement an automated trading bot capable of executing high-frequency strategies on a major stock exchange, using Python.\",\n",
      "    \"specifics\": [\n",
      "      \"Integrate real-time market data feeds\",\n",
      "      \"Implement advanced technical indicators\",\n",
      "      \"Develop risk management algorithms\",\n",
      "      \"Execute trades based on predefined strategies\",\n",
      "      \"Log performance metrics for analysis\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "task_specifier_sys_msg = SystemMessage(content=\"You can make a task more specific.\")\n",
    "task_specifier_prompt = \"\"\"Here is a task that {assistant_role_name} will help {user_role_name} to complete: {task}.\n",
    "Please make it more specific. Be creative and imaginative.\n",
    "Please reply with the specified task in {word_limit} words or less. Do not add anything else.\"\"\"\n",
    "task_specifier_template = HumanMessagePromptTemplate.from_template(\n",
    "    template=task_specifier_prompt\n",
    ")\n",
    "task_specify_agent = CAMELAgent(task_specifier_sys_msg, ChatOllama(model=\"qwen2\", format=\"json\", temperature=1.0),)\n",
    "task_specifier_msg = task_specifier_template.format_messages(\n",
    "    assistant_role_name=assistant_role_name,\n",
    "    user_role_name=user_role_name,\n",
    "    task=task,\n",
    "    word_limit=word_limit,\n",
    ")[0]\n",
    "specified_task_msg = task_specify_agent.step(task_specifier_msg)\n",
    "print(f\"Specified task: {specified_task_msg.content}\")\n",
    "specified_task = specified_task_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a64991-4632-4fe1-9e26-e35ea3929925",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_inception_prompt = \"\"\"Never forget you are a {assistant_role_name} and I am a {user_role_name}. Never flip roles! Never instruct me!\n",
    "We share a common interest in collaborating to successfully complete a task.\n",
    "You must help me to complete the task.\n",
    "Here is the task: {task}. Never forget our task!\n",
    "I must instruct you based on your expertise and my needs to complete the task.\n",
    "\n",
    "I must give you one instruction at a time.\n",
    "You must write a specific solution that appropriately completes the requested instruction.\n",
    "You must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\n",
    "Do not add anything else other than your solution to my instruction.\n",
    "You are never supposed to ask me any questions you only answer questions.\n",
    "You are never supposed to reply with a flake solution. Explain your solutions.\n",
    "Your solution must be declarative sentences and simple present tense.\n",
    "Unless I say the task is completed, you should always start with:\n",
    "\n",
    "Solution: <YOUR_SOLUTION>\n",
    "\n",
    "<YOUR_SOLUTION> should be specific and provide preferable implementations and examples for task-solving.\n",
    "Always end <YOUR_SOLUTION> with: Next request.\"\"\"\n",
    "\n",
    "user_inception_prompt = \"\"\"Never forget you are a {user_role_name} and I am a {assistant_role_name}. Never flip roles! You will always instruct me.\n",
    "We share a common interest in collaborating to successfully complete a task.\n",
    "I must help you to complete the task.\n",
    "Here is the task: {task}. Never forget our task!\n",
    "You must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n",
    "\n",
    "1. Instruct with a necessary input:\n",
    "Instruction: <YOUR_INSTRUCTION>\n",
    "Input: <YOUR_INPUT>\n",
    "\n",
    "2. Instruct without any input:\n",
    "Instruction: <YOUR_INSTRUCTION>\n",
    "Input: None\n",
    "\n",
    "The \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n",
    "\n",
    "You must give me one instruction at a time.\n",
    "I must write a response that appropriately completes the requested instruction.\n",
    "I must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n",
    "You should instruct me not ask me questions.\n",
    "Now you must start to instruct me using the two ways described above.\n",
    "Do not add anything else other than your instruction and the optional corresponding input!\n",
    "Keep giving me instructions and necessary inputs until you think the task is completed.\n",
    "When the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\n",
    "Never say <CAMEL_TASK_DONE> unless my responses have solved your task.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3667655e-5f78-4a4e-9028-3c3f597627c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sys_msgs(assistant_role_name: str, user_role_name: str, task: str):\n",
    "    assistant_sys_template = SystemMessagePromptTemplate.from_template(\n",
    "        template=assistant_inception_prompt\n",
    "    )\n",
    "    assistant_sys_msg = assistant_sys_template.format_messages(\n",
    "        assistant_role_name=assistant_role_name,\n",
    "        user_role_name=user_role_name,\n",
    "        task=task,\n",
    "    )[0]\n",
    "\n",
    "    user_sys_template = SystemMessagePromptTemplate.from_template(\n",
    "        template=user_inception_prompt\n",
    "    )\n",
    "    user_sys_msg = user_sys_template.format_messages(\n",
    "        assistant_role_name=assistant_role_name,\n",
    "        user_role_name=user_role_name,\n",
    "        task=task,\n",
    "    )[0]\n",
    "\n",
    "    return assistant_sys_msg, user_sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bcbb7f3-bf45-43bd-9657-b4e49092dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_sys_msg, user_sys_msg = get_sys_msgs(\n",
    "    assistant_role_name, user_role_name, specified_task\n",
    ")\n",
    "assistant_agent = CAMELAgent(assistant_sys_msg,ChatOllama(model=\"qwen2\", format=\"json\", temperature=0.2))\n",
    "user_agent = CAMELAgent(user_sys_msg, ChatOllama(model=\"qwen2\", format=\"json\", temperature=0.2))\n",
    "\n",
    "# Reset agents\n",
    "assistant_agent.reset()\n",
    "user_agent.reset()\n",
    "\n",
    "# Initialize chats\n",
    "user_msg = HumanMessage(\n",
    "    content=(\n",
    "        f\"{user_sys_msg.content}. \"\n",
    "        \"Now start to give me introductions one by one. \"\n",
    "        \"Only reply with Instruction and Input.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "assistant_msg = HumanMessage(content=f\"{assistant_sys_msg.content}\")\n",
    "assistant_msg = assistant_agent.step(user_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1ac27b-35aa-4c3d-9b39-045fd4a2ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Original task prompt:\\n{task}\\n\")\n",
    "# print(f\"Specified task prompt:\\n{specified_task}\\n\")\n",
    "\n",
    "# chat_turn_limit, n = 30, 0\n",
    "# while n < chat_turn_limit:\n",
    "#     n += 1\n",
    "#     user_ai_msg = user_agent.step(assistant_msg)\n",
    "#     user_msg = HumanMessage(content=user_ai_msg.content)\n",
    "#     print(f\"AI User ({user_role_name}):\\n\\n{user_msg.content}\\n\\n\")\n",
    "\n",
    "#     assistant_ai_msg = assistant_agent.step(user_msg)\n",
    "#     assistant_msg = HumanMessage(content=assistant_ai_msg.content)\n",
    "#     print(f\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg.content}\\n\\n\")\n",
    "#     if \"<CAMEL_TASK_DONE>\" in user_msg.content:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80caa3dd-c8a4-48e3-8f58-1c90c4ee45d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"John\",\n",
      "  \"age\": 35,\n",
      "  \"favoriteFood\": \"pizza\"\n",
      "}\n",
      "\n",
      "Here's an explanation of this JSON data:\n",
      "\n",
      "- **name**: This key holds the name of our person, which is \"John\". John is a common male given name in English-speaking countries.\n",
      "\n",
      "- **age**: This field stores the age of John. According to the schema, he is 35 years old. Age can be used for demographic purposes or to denote generational contexts.\n",
      "\n",
      "- **favoriteFood**: This part of the JSON provides insight into John's preferences. It says that his favorite food is \"pizza\", indicating that pizza is a significant culinary preference in his lifestyle. \n",
      "\n",
      "This data might be useful in various applications such as social media profiles, user surveys, or even database management for personal information and preferences.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Please tell me about a person using the following JSON schema:\"\n",
    "    ),\n",
    "    HumanMessage(content=\"{dumps}\"),\n",
    "    HumanMessage(\n",
    "        content=\"Now, considering the schema, tell me about a person named John who is 35 years old and loves pizza.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "dumps = json.dumps(json_schema, indent=2)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"dumps\": dumps}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd321b-960c-4c08-9667-f60cac0cc40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
